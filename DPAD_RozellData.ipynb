{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!python --version\n",
    "# !sudo update-alternatives --config python3\n",
    "\n",
    "%pip install --upgrade DPAD # Install or update to the latest version of DPAD\n",
    "\n",
    "# Install dependencies only required for this notebook and other development work\n",
    "%pip install sympy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing for Gordon Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "y (neural) data:\n",
    "    - \"theta coherence\" has 1 dimension, and \"raw vHPC\" + \"mPFC LFP\" have 2 dimensions, for a total shape of (14123335, 3)\n",
    "\n",
    "z (behavioral) data:\n",
    "    - \"correct\" has 1 dimension for a total shape of (14123335, 1)\n",
    "\n",
    "u (input) data:\n",
    "    - \"show\", \"delay\", \"test\", and \"rest\" have 4 dimensions, and \"control_input\" has 1 dimension, for a total shape of (14123335, 5)\n",
    "\"\"\"\n",
    "\n",
    "# Load the theta coherence data and behavioral data\n",
    "theta_data = pd.read_csv('theta_coherence_with_behavior.csv', delim_whitespace=True)\n",
    "\n",
    "lfp_data = pd.read_csv('filtered_lfp.csv', delim_whitespace=True)\n",
    "# Combine theta coherence data with LFP data along columns (axis=1)\n",
    "combined_data = pd.concat([theta_data, lfp_data], axis=1)\n",
    "\n",
    "# Define the columns for y (neural data), u (control input), and z (behavioral data)\n",
    "y_columns = ['Theta_coherence', 'vHPC_LFP', 'mPFC_LFP']  # Replace with actual y column names\n",
    "z_column = ['Correct']  # Replace with the actual behavioral column name for z\n",
    "u_columns = [\"Show\", \"Delay\", \"Test\", \"Rest\", \"Control_input\"]\n",
    "\n",
    "# Split data into y, u, and z inputs\n",
    "y_data = combined_data[y_columns]\n",
    "u_data = combined_data[u_columns]\n",
    "z_data = combined_data[z_column]\n",
    "\n",
    "# Perform train-test split (80:20) for just y, u, and z inputs, without targets\n",
    "yTrain, yTest, uTrain, uTest, zTrain, zTest = train_test_split(\n",
    "    y_data, u_data, z_data, train_size=0.8, shuffle=False\n",
    ")\n",
    "\n",
    "print(yTrain.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing for Stanley Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynwb import NWBHDF5IO\n",
    "\n",
    "\"\"\"\n",
    "y (neural) data:\n",
    "    - using LFP1 which has shape dimensions (16749229, 32)\n",
    "    - LFP2 also exists but documentation does not mention it\n",
    "    - sampling rate of 2034.5052083 Hz\n",
    "\n",
    "z (behavioral) data:\n",
    "    - whisking (0 or 1) has 1 dimension and whisker motion has 1 dimension, for a total shape of (238248, 2)\n",
    "    - time stamp starts at 5.0358272 s and increases 0.0339968 s, for a sampling rate of â‰ˆ 29.413 Hz\n",
    "\n",
    "u (opto input) data:\n",
    "    - u_opto had 1 dimension and u_galvo has 1 dimension, for a total shape of (16749227, 2)\n",
    "\n",
    "* all three variables have different sizes!\n",
    "* I can downsample y and u arrays to the length of z (currently commented bellow) but not sure how that affects results\n",
    "\"\"\"\n",
    "\n",
    "io = NWBHDF5IO('./Stanley_whisk/AB020_1_Proc.nwb', mode=\"r\")\n",
    "nwbfile = io.read()\n",
    "\n",
    "y_lfp1 = nwbfile.processing['ecephys'].data_interfaces['LFP1'].electrical_series['ElectricalSeries'].data[:] * float(1000000) # convert to microvolts\n",
    "#y_lfp2 = nwbfile.processing['ecephys'].data_interfaces['LFP2'].electrical_series['ElectricalSeries'].data[:] * float(1000000) # convert to microvolts \n",
    "#y_data = np.concatenate((y_lfp1, y_lfp2), axis = 1)\n",
    "y_data = y_lfp1\n",
    "\n",
    "u_opto = nwbfile.acquisition['OptogeneticSeries1'].data[:]\n",
    "u_opto = np.reshape(u_opto, (u_opto.shape[0], 1))\n",
    "u_galvo = nwbfile.acquisition['GalvoSeries1'].data[:]\n",
    "u_galvo = np.reshape(u_galvo, (u_galvo.shape[0], 1))\n",
    "u_data = np.concatenate((u_opto, u_galvo), axis = 1)\n",
    "\n",
    "z_whisking = nwbfile.processing['whisker'].data_interfaces['Whisking'].data[:]\n",
    "print(nwbfile.processing['whisker'].data_interfaces['Whisking'])\n",
    "print(nwbfile.processing['whisker'].data_interfaces['Whisking'].timestamps[:5])\n",
    "z_whisking = np.reshape(z_whisking, (z_whisking.shape[0], 1))\n",
    "z_whiskerMotion = nwbfile.processing['whisker'].data_interfaces['WhiskerMotion'].data[:]\n",
    "z_whiskerMotion = np.reshape(z_whiskerMotion, (z_whiskerMotion.shape[0], 1))\n",
    "z_data = np.concatenate((z_whisking, z_whiskerMotion), axis = 1)\n",
    "\n",
    "print(y_data.shape)\n",
    "print(z_data.shape)\n",
    "print(u_data.shape)\n",
    "\n",
    "#Downsampling to the size of z:\n",
    "downsampled_y = y_data[::69]\n",
    "downsampled_u = u_data[::69]\n",
    "\n",
    "target_size = 238248 #size of z\n",
    "aligned_y = downsampled_y[:target_size] if downsampled_y.size > target_size else np.pad(downsampled_y, (0, target_size - downsampled_y.size), mode='constant')\n",
    "aligned_u = downsampled_u[:target_size] if downsampled_u.size > target_size else np.pad(downsampled_u, (0, target_size - downsampled_u.size), mode='constant')\n",
    "y_data = aligned_y\n",
    "u_data = aligned_u\n",
    "\n",
    "yTrain, yTest, uTrain, uTest, zTrain, zTest = train_test_split(\n",
    "    y_data, u_data, z_data, train_size=0.8, shuffle=False\n",
    ")\n",
    "\n",
    "print(yTrain.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting to DPAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DPAD import DPADModel\n",
    "\n",
    "\n",
    "idSys = DPADModel()\n",
    "methodCode = \"DPAD_CzNonLin\"\n",
    "args = DPADModel.prepare_args(methodCode)\n",
    "\n",
    "idSys.fit(Y = yTrain.T, Z = zTrain.T, nx = 6, n1 = 3, **args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (ignore for now) Preprocessing Stanley Data in case we want target variables too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the theta coherence data and behavioral data\n",
    "theta_data = pd.read_csv('theta_coherence_with_behavior.csv')\n",
    "lfp_data = pd.read_csv('filtered_lfp.csv')\n",
    "\n",
    "# Combine theta coherence data with LFP data along columns (axis=1)\n",
    "combined_data = pd.concat([theta_data, lfp_data], axis=1)\n",
    "\n",
    "# Define the columns for y (neural data) and z (behavioral data)\n",
    "y_columns = ['Theta_coherence', 'vHPC_LFP', 'mPFC_LFP'] \n",
    "z_column = ['Correct'] \n",
    "u_columns = [\"Show\", \"Delay\", \"Test\", \"Rest\", \"Control_input\"]\n",
    "\n",
    "# Create target columns by shifting y and z data by one time step\n",
    "for col in y_columns:\n",
    "    combined_data[f'target_{col}'] = combined_data[col].shift(-1)\n",
    "\n",
    "# Shift the behavioral data by one time step to create target_z\n",
    "combined_data['target_z'] = combined_data[z_column[0]].shift(-1)\n",
    "\n",
    "# Drop the last row, as it will have NaNs after shifting\n",
    "combined_data = combined_data.dropna()\n",
    "\n",
    "# Split data into y, u, and z inputs\n",
    "y_data = combined_data[y_columns]\n",
    "u_data = combined_data[u_columns]\n",
    "z_data = combined_data[z_column]\n",
    "\n",
    "# Define targets for y and z separately\n",
    "y_targets = combined_data[[f'target_{col}' for col in y_columns]]\n",
    "z_targets = combined_data['target_z']\n",
    "\n",
    "# Perform train-test split (80:20) while keeping y, u, and z targets separate\n",
    "y_training, y_testing, u_training, u_testing, z_training, z_testing, target_y_training, target_y_testing, target_z_training, target_z_testing = train_test_split(\n",
    "    y_data, u_data, z_data, y_targets, z_targets, train_size=0.8, shuffle=False\n",
    ")\n",
    "\n",
    "# Convert `target_y_training` and `target_y_testing` to arrays with multiple dimensions\n",
    "target_y_training = target_y_training.values  # Shape will be (num_train_samples, len(y_columns))\n",
    "target_y_testing = target_y_testing.values    # Shape will be (num_test_samples, len(y_columns))\n",
    "\n",
    "# Convert `target_z_training` and `target_z_testing` to 1D arrays\n",
    "target_z_training = target_z_training.values  # Shape will be (num_train_samples,)\n",
    "target_z_testing = target_z_testing.values    # Shape will be (num_test_samples,)\n",
    "\n",
    "# Verify the shapes\n",
    "print(\"y_training shape:\", y_training.shape)\n",
    "print(\"y_testing shape:\", y_testing.shape)\n",
    "print(\"target_y_training shape:\", target_y_training.shape)  # Multi-dimensional\n",
    "print(\"target_y_testing shape:\", target_y_testing.shape)\n",
    "print(\"u_training shape:\", u_training.shape)\n",
    "print(\"u_testing shape:\", u_testing.shape)\n",
    "print(\"z_training shape:\", z_training.shape)\n",
    "print(\"z_testing shape:\", z_testing.shape)\n",
    "print(\"target_z_training shape:\", target_z_training.shape)  # Single-dimensional\n",
    "print(\"target_z_testing shape:\", target_z_testing.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DPAD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
